1.kafka的优势
  a.快速:单一的Kafka代理可以处理成千上万的客户端，每秒处理数兆字节的读写操作。
  b.可伸缩:在一组机器上对数据进行分区和简化，以支持更大的数据
  c.持久:消息是持久性的，并在集群中进行复制，以防止数据丢失
  d.它提供了容错保证和持久性
2.为什么需要消息系统
  a.解耦：允许你独立的扩展或修改两边的处理过程，只要确保它们遵守同样的接口约束。
  b.冗余:消息队列把数据进行持久化直到它们已经被完全处理，通过这一方式规避了数据丢失风险。许多消息队列所采用的”插入-获取-删除”范式中，在把一个消息从队列中删除之前，需要你的处理系统明确的指出该消息已经被处理完毕，从而确保你的数据被安全的保存直到你使用完毕。
  c.扩展性：因为消息队列解耦了你的处理过程，所以增大消息入队和处理的频率是很容易的，只要另外增加处理过程即可。
  d.灵活性 & 峰值处理能力：在访问量剧增的情况下，应用仍然需要继续发挥作用，但是这样的突发流量并不常见。如果为以能处理这类峰值访问为标准来投入资源随时待命无疑是巨大的浪费。使用消息队列能够使关键组件顶住突发的访问压力，而不会因为突发的超负荷的请求而完全崩溃。
  e.可恢复性：系统的一部分组件失效时，不会影响到整个系统。消息队列降低了进程间的耦合度，所以即使一个处理消息的进程挂掉，加入队列中的消息仍然可以在系统恢复后被处理。
  f.顺序保证：在大多使用场景下，数据处理的顺序都很重要。大部分消息队列本来就是排序的，并且能保证数据会按照特定的顺序来处理。（Kafka 保证一个 Partition 内的消息的有序性）
  g.缓冲：有助于控制和优化数据流经过系统的速度，解决生产消息和消费消息的处理速度不一致的情况。
  k.异步通信：很多时候，用户不想也不需要立即处理消息。消息队列提供了异步处理机制，允许用户把一个消息放入队列，但并不立即处理它。想向队列中放入多少消息就放多少，然后在需要的时候再去处理它们。
3.producer发布信息
  a.写入方式
    producer采用push模式将消息发送到broker，每条消息都被追加到partition中，属于顺序写入磁盘
  b.消息路由
    producer发送消息到broker时，会根据分区算法选择将其存储到哪一个分区，其路由机制:
      1.指定分区，则直接使用
      2.未指定分区，但指定key，通过对key的value进行hash选出一个partition
      3.partition和key都没有指定，使用轮询选出一个分区
  c.写入流程
    1.producer先从zookeeper的"/brokers/.../state" 节点找到该 partition 的 leader
    2.producer将消息发送给leader
    3.leader将消息写入本地log
    4.followers从leader拉取消息，写入本地log后leader发送ack
    5.leader收到所有ISR中的replica 的 ACK 后，增加 HW（high watermark，最后 commit 的 offset） 并向 producer 发送 ACK
  d.producer delivery guarantee
    1.At most once 消息可能会丢，但绝不会重复传输
    2.At least one 消息绝不会丢，但可能会重复传输
    3.Exactly once 每条消息肯定会被传输一次且仅传输一次
      当 producer 向 broker 发送消息时，一旦这条消息被 commit，由于 replication 的存在，它就不会丢。但是如果 producer 发送数据给 broker 后，遇到网络问题而造成通信中断，那 Producer 就无法判断该条消息是否已经 commit。虽然 Kafka 无法确定网络故障期间发生了什么，但是 producer 可以生成一种类似于主键的东西，发生故障时幂等性的重试多次，这样就做到了 Exactly once，但目前还并未实现。所以目前默认情况下一条消息从 producer 到 broker 是确保了 At least once，可通过设置 producer 异步发送实现At most once。
4.broker保存消息
  a.存储方式
    物理上把topic分成一个或多个分区，每个分区对应一个文件夹（该文件夹存储该分区的所有消息和索引）
  b.存储策略
    无论消息是否被消费，kafka都会保留所有消息。有两种策略可以删除旧数据:基于时间：log.retention.hours=168;基于大小：log.retention.bytes=1073741824;
    注意：因为Kafka读取特定消息的时间复杂度为O(1)，即与文件大小无关，所以这里删除过期文件与提高 Kafka 性能无关。
  c.topic 创建与删除
    1.创建topic
      a.controller 在 ZooKeeper 的 /brokers/topics 节点上注册 watcher，当 topic 被创建，则 controller 会通过 watch 得到该 topic 的 partition/replica 分配。
      b.controller从 /brokers/ids 读取当前所有可用的 broker 列表，对于 set_p 中的每一个 partition：
        b.1 从分配给该 partition 的所有 replica（称为AR）中任选一个可用的 broker 作为新的 leader，并将AR设置为新的 ISR
        b.2 将新的 leader 和 ISR 写入 /brokers/topics/[topic]/partitions/[partition]/state
      c. controller 通过 RPC 向相关的 broker 发送 LeaderAndISRRequest。
    2.删除 topic
      a. controller 在 zooKeeper 的 /brokers/topics 节点上注册 watcher，当 topic 被删除，则 controller 会通过 watch 得到该 topic 的 partition/replica 分配。
      b. 若 delete.topic.enable=false，结束；否则 controller 注册在 /admin/delete_topics 上的 watch 被 fire，controller 通过回调向对应的 broker 发送 StopReplicaRequest。
5.kafka ha
  a.replication(副本)
    同一个 partition 可能会有多个 replica（对应 server.properties 配置中的 default.replication.factor=N）。没有 replica 的情况下，一旦 broker 宕机，其上所有 patition 的数据都不可被消费，同时 producer 也不能再将数据存于其上的 patition。引入replication 之后，同一个 partition 可能会有多个 replica，而这时需要在这些 replica 之间选出一个 leader，producer 和 consumer 只与这个 leader 交互，其它 replica 作为 follower 从 leader 中复制数据。
    kafka分配replica的算法:
      1.将所有 broker（假设共 n 个 broker）和待分配的 partition 排序
      2.将第 i 个 partition 分配到第（i mod n）个 broker 上
      3. 将第 i 个 partition 的第 j 个 replica 分配到第（(i + j) mode n）个 broker上
  b.leader failover
    当 partition 对应的 leader 宕机时，需要从 follower 中选举出新 leader。在选举新leader时，一个基本的原则是，新的 leader 必须拥有旧 leader commit 过的所有消息。
    kafka 在 zookeeper 中（/brokers/…/state）动态维护了一个 ISR（in-sync replicas），由3.3节的写入流程可知 ISR 里面的所有 replica 都跟上了 leader，只有 ISR 里面的成员才能选为 leader。对于 f+1 个 replica，一个 partition 可以在容忍 f 个 replica 失效的情况下保证消息不丢失。
    当所有 replica 都不工作时，有两种可行的方案：
      1. 等待 ISR 中的任一个 replica 活过来，并选它作为 leader。可保障数据不丢失，但时间可能相对较长。
      2. 选择第一个活过来的 replica（不一定是 ISR 成员）作为 leader。无法保障数据不丢失，但相对不可用时间较短。
  c.broker failover
    1.controller 在 zookeeper 的 /brokers/ids/[brokerId] 节点注册 Watcher，当 broker 宕机时 zookeeper 会 fire watch
    2.controller 从 /brokers/ids 节点读取可用broker
    3. controller决定set_p，该集合包含宕机 broker 上的所有 partition
    4.对 set_p 中的每一个 partition
      4.1从/brokers/topics/[topic]/partitions/[partition]/state 节点读取 ISR
      4.2决定新 leader
      4.3将新 leader、ISR、controller_epoch 和 leader_epoch 等信息写入 state 节点
    5.通过 RPC 向相关 broker 发送 leaderAndISRRequest 命令
  d.controller failover
    当 controller 宕机时会触发 controller failover。每个 broker 都会在 zookeeper 的 “/controller” 节点注册 watcher，当 controller 宕机时 zookeeper 中的临时节点消失，所有存活的 broker 收到 fire 的通知，每个 broker 都尝试创建新的 controller path，只有一个竞选成功并当选为 controller。

    当新的 controller 当选时，会触发 KafkaController.onControllerFailover 方法，在该方法中完成如下操作：
      1. 读取并增加 Controller Epoch。
      2. 在 reassignedPartitions Patch(/admin/reassign_partitions) 上注册 watcher。
      3. 在 preferredReplicaElection Path(/admin/preferred_replica_election) 上注册 watcher。
      4. 通过 partitionStateMachine 在 broker Topics Patch(/brokers/topics) 上注册 watcher。
      5. 若 delete.topic.enable=true（默认值是 false），则 partitionStateMachine 在 Delete Topic Patch(/admin/delete_topics) 上注册 watcher。
      6. 通过 replicaStateMachine在 Broker Ids Patch(/brokers/ids)上注册Watch。
      7. 初始化 ControllerContext 对象，设置当前所有 topic，“活”着的 broker 列表，所有 partition 的 leader 及 ISR等。
      8. 启动 replicaStateMachine 和 partitionStateMachine。
      9. 将 brokerState 状态设置为 RunningAsController。
      10. 将每个 partition 的 Leadership 信息发送给所有“活”着的 broker。
      11. 若 auto.leader.rebalance.enable=true（默认值是true），则启动 partition-rebalance 线程。
      12. 若 delete.topic.enable=true 且Delete Topic Patch(/admin/delete_topics)中有值，则删除相应的Topic。
6.consumer 消费消息
  a.The high-level consumer API
    high-level consumer API 提供了 consumer group 的语义，一个消息只能被 group 内的一个 consumer 所消费，且 consumer 消费消息时不关注 offset，最后一个 offset 由 zookeeper 保存。
    使用 high-level consumer API 可以是多线程的应用，应当注意：
      1.如果消费线程大于 patition 数量，则有些线程将收不到消息
      2.如果 patition 数量大于线程数，则有些线程多收到多个 patition 的消息
      3. 如果一个线程消费多个 patition，则无法保证你收到的消息的顺序，而一个 patition 内的消息是有序的
  b.The SimpleConsumer API
    如果你想要对 patition 有更多的控制权，那就应该使用 SimpleConsumer API，比如：
      1.多次读取一个消息
      2.只消费一个 patition 中的部分消息
      3.使用事务来保证一个消息仅被消费一次
    但是使用此 API 时，partition、offset、broker、leader 等对你不再透明，需要自己去管理。你需要做大量的额外工作：
      1. 必须在应用程序中跟踪 offset，从而确定下一条应该消费哪条消息
      2.应用程序需要通过程序获知每个 Partition 的 leader 是谁
      3.需要处理 leader 的变更
    使用 SimpleConsumer API 的一般流程如下：
      1.查找到一个“活着”的 broker，并且找出每个 partition 的 leader
      2.找出每个 partition 的 follower
      3.定义好请求，该请求应该能描述应用程序需要哪些数据
      4.fetch 数据
      5.识别 leader 的变化，并对之作出必要的响应
  c.consumer group
     kafka 的分配单位是 patition。每个 consumer 都属于一个 group，一个 partition 只能被同一个 group 内的一个 consumer 所消费（也就保障了一个消息只能被 group 内的一个 consuemr 所消费），但是多个 group 可以同时消费这个 partition。
     kafka 的设计目标之一就是同时实现离线处理和实时处理，根据这一特性，可以使用 spark/Storm 这些实时处理系统对消息在线处理，同时使用 Hadoop 批处理系统进行离线处理，还可以将数据备份到另一个数据中心，只需要保证这三者属于不同的 consumer group。
  d.消费方式
    consumer 采用 pull 模式从 broker 中读取数据。push 模式很难适应消费速率不同的消费者，因为消息发送速率是由 broker 决定的。它的目标是尽可能以最快速度传递消息，但是这样很容易造成 consumer 来不及处理消息，典型的表现就是拒绝服务以及网络拥塞。而 pull 模式则可以根据 consumer 的消费能力以适当的速率消费消息。对于 Kafka 而言，pull 模式更合适，它可简化 broker 的设计，consumer 可自主控制消费消息的速率，同时 consumer 可以自己控制消费方式——即可批量消费也可逐条消费，同时还能选择不同的提交方式从而实现不同的传输语义。
  e.consumer delivery guarantee
    如果将 consumer 设置为 autocommit，consumer 一旦读到数据立即自动 commit。如果只讨论这一读取消息的过程，那 Kafka 确保了 Exactly once。
    但实际使用中应用程序并非在 consumer 读取完数据就结束了，而是要进行进一步处理，而数据处理与 commit 的顺序在很大程度上决定了consumer delivery guarantee：
      1.读完消息先 commit 再处理消息。这种模式下，如果 consumer 在 commit 后还没来得及处理消息就 crash 了，下次重新开始工作后就无法读到刚刚已提交而未处理的消息，这就对应于 At most once
      2.读完消息先处理再 commit。这种模式下，如果在处理完消息之后 commit 之前 consumer crash 了，下次重新开始工作时还会处理刚刚未 commit 的消息，实际上该消息已经被处理过了。这就对应于 At least once。
      3.如果一定要做到 Exactly once，就需要协调 offset 和实际操作的输出.精典的做法是引入两阶段提交。如果能让 offset 和操作输入存在同一个地方，会更简洁和通用。这种方式可能更好，因为许多输出系统可能不支持两阶段提交。比如，consumer 拿到数据后可能把数据放到 HDFS，如果把最新的 offset 和数据本身一起写到 HDFS，那就可以保证数据的输出和 offset 的更新要么都完成，要么都不完成，间接实现 Exactly once。（目前就 high-level API而言，offset 是存于Zookeeper 中的，无法存于HDFS，而SimpleConsuemr API的 offset 是由自己去维护的，可以将之存于 HDFS 中）
    如果希望能够严格的不丢数据，解决办法有两个：
      手动commit offset，并针对partition_num启同样数目的consumer进程，这样就能保证一个consumer进程占有一个partition，commit offset的时候不会影响别的partition的offset。但这个方法比较局限，因为partition和consumer进程的数目必须严格对应。
      另一个方法同样需要手动commit offset，另外在consumer端再将所有fetch到的数据缓存到queue里，当把queue里所有的数据处理完之后，再批量提交offset，这样就能保证只有处理完的数据才被commit。当然这只是基本思路，实际上操作起来不是这么简单，具体做法以后我再另开一篇。
  f.consumer rebalance
    使用bootstrap.servers替代之前版本的zookeeper.connect，相关的有如下两个改动：
    在 Server 端增加了 GroupCoordinator 这个角色
    将 topic 的 offset 信息由之前存储在 zookeeper(/consumers/<group.id>/offsets/<topic>/<partitionId>,zk写操作性能不高) 上改为存储到一个特殊的 topic 中（__consumer_offsets）

    Coordinator一般指的是运行在broker上的group Coordinator，用于管理Consumer Group中各个成员，每个KafkaServer都有一个GroupCoordinator实例，管理多个消费者组，主要用于offset位移管理和Consumer Rebalance。

    consumer rebalance算法如下：
      1. 将目标 topic 下的所有 partirtion 排序，存于PT
      2. 对某 consumer group 下所有 consumer 排序，存于 CG，第 i 个consumer 记为 Ci
      3. N=size(PT)/size(CG)，向上取整
      4. 解除 Ci 对原来分配的 partition 的消费权（i从0开始）
      5. 将第i*N到（i+1）*N-1个 partition 分配给 Ci





10.日志清除策略
  Kafka日志管理器允许定制删除策略。目前的策略是删除修改时间在N天之前的日志（按时间删除），也可以使用另外一个策略：保留最后的N GB数据的策略(按大小删除)。为了避免在删除时阻塞读操作，采用了copy-on-write(CopyOnWrite机制称为写时复制，理解起来很简单，就是执行修改操作时进行底层数组复制，使得修改操作在新的数组上进行，不妨碍原数组的并发读操作，复制修改完成后更新原数组引用变量。)形式的实现，删除操作进行时，读取操作的二分查找功能实际是在一个静态的快照副本上进行的
    log.cleanup.policy=delete启用删除策略
    直接删除，删除后的消息不可恢复。可配置以下两个策略：
    清理超过指定时间清理：
    log.retention.hours=16
    超过指定大小后，删除旧的消息：
    log.retention.bytes=1073741824
  压缩策略:
    将数据压缩，只保留每个key最后一个版本的数据。首先在broker的配置中设置log.cleaner.enable=true启用cleaner，这个默认是关闭的。在Topic的配置中设置log.cleanup.policy=compact启用压缩策略。
